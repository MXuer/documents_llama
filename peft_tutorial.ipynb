{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf2e2d3",
   "metadata": {},
   "source": [
    "## Lora Model Training by PEFT\n",
    "\n",
    "PEFT: Parameters-Efficient Fine-Tuning \n",
    "\n",
    "- Supported Models:\n",
    "    - CasualLM\n",
    "    - Seq2SeqLM\n",
    "    - SequenceClassification\n",
    "    - TokenClassification\n",
    "    \n",
    "- Supported PEFT Methods:\n",
    "    - PromptEmbedding\n",
    "    - PromptEncoder\n",
    "    - PrefixTuning\n",
    "    - Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d3ce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duhu/anaconda3/envs/llm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541464f3",
   "metadata": {},
   "source": [
    "## Lora\n",
    "\n",
    "- lora仅支持`nn.Linear`，所以需要确定要使用lora的层的属性；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa139ac3",
   "metadata": {},
   "source": [
    "### LoraConfig\n",
    "\n",
    "输入的参数：\n",
    "- r: lora attention dimesions, default: 8\n",
    "- taget_modules: str or list of string, the name of the modules to apply lora to. 默认不指定module，如果要指定的话，可以输入base model的module name，这个需要自己打印出来base model的结构才能确定是啥；不指定的时候会在内置的一个字典里面去找，这里包含了市面上的大部分常用的预训练模型，以及其对应的可以低秩分解：\n",
    "```json\n",
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\n",
    "    \"t5\": [\"q\", \"v\"],\n",
    "    \"mt5\": [\"q\", \"v\"],\n",
    "    \"bart\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt2\": [\"c_attn\"],\n",
    "    \"bloom\": [\"query_key_value\"],\n",
    "    \"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\n",
    "    \"opt\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gptj\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt_neox\": [\"query_key_value\"],\n",
    "    \"gpt_neo\": [\"q_proj\", \"v_proj\"],\n",
    "    \"bert\": [\"query\", \"value\"],\n",
    "    \"roberta\": [\"query\", \"value\"],\n",
    "    \"xlm-roberta\": [\"query\", \"value\"],\n",
    "    \"electra\": [\"query\", \"value\"],\n",
    "    \"deberta-v2\": [\"query_proj\", \"value_proj\"],\n",
    "    \"deberta\": [\"in_proj\"],\n",
    "    \"layoutlm\": [\"query\", \"value\"],\n",
    "    \"llama\": [\"q_proj\", \"v_proj\"],\n",
    "    \"chatglm\": [\"query_key_value\"],\n",
    "}\n",
    "```\n",
    "- lora_alpha: the alpha paramter for Lora scaling **Explanation Later**\n",
    "- lora_dropout: the dropout of Lora\n",
    "- fan_in_fan_out: Set this to True if the layer to replace stores weight like (fan_in, fan_out) ***EL**\n",
    "- bias: Bias type for lora, 'none', 'all' or 'lora_only'\n",
    "- module_to_save: List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint.除了涉及到lora的部分，剩下的还有哪些模块需要参与训练，最后也会保存到最终的checkpoint里，比如说用预训练模型去做一个分类任务，最后一层（分类层）的参数也是需要训练和保存的。\n",
    "\n",
    "内嵌的属性：\n",
    "- peft_type: PeftType.Lora "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba5f62",
   "metadata": {},
   "source": [
    "### ~~GPT-2 as an example~~ \n",
    "\n",
    "GPT-2的attention中的q/k/v的转换用的不是线性层，而是conv1d……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59a796",
   "metadata": {},
   "source": [
    "### ~~GPT as an example~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f3a36",
   "metadata": {},
   "source": [
    "### EleutherAI/gpt-neo-1.3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b88424",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "570d0e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 2048)\n",
       "    (wpe): Embedding(2048, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c36abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, # 不指定任务模型的类别的时候，返回的是PeftModel\n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=.1, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86dfeaf",
   "metadata": {},
   "source": [
    "- 此处我们指定的任务类别是`CAUSAL_LM`，所以返回的model是`PeftModelForCausalLM`\n",
    "- `PeftModelForCausalLM`是继承自`PeftModel`的\n",
    "- `PeftModel`封装了通用的一些方法，`PeftModelForCausalLM`定制化一些专对CausalLM的一些方法；\n",
    "- `PeftModel`有个初始化参数，`adapter_name`，默认值是`default`，这个参数是为了基于同一个base model搞出来多个adapter，想要切换adapter，可以直接根据这个参数来切换，具体的逻辑和作用可以参考[Multi Adapter support](https://github.com/huggingface/peft/pull/263#issue-1654639358)。主要目的就是：use multi-adapter at same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1ecf4",
   "metadata": {},
   "source": [
    "`Peft`的一些方法：\n",
    "- 如果Peft模型的类别是Lora的话，PeftModel类的初始化方法里面的base_model是LoraModel，PROMPT_TUNING/P_TUNING/PREFIX_TUNING的base model还是预训练的模型；\n",
    "- set_additional_trainable_modules(self, peft_config, adapter_name)\n",
    "    - `PeftConfig`中的`modules_to_save`不为None的时候(比如基于base model训练一个分类器，最后一层的module应该是要保存的)，就把这些modules添加到self.modules_to_save，同时设定这些modules可训练； 比如分类器中，modules_to_save的设定为\n",
    "    ```python\n",
    "    self.modules_to_save = {\"classifier\", \"score\"}\n",
    "    ```\n",
    "- add_adapter(self, adapter_name, peft_config)\n",
    "    - 如果`PeftConfig`的基类是`PromptLearningConfig`(也就是PROMPT_TUNING/P_TUNING/PREFIX_TUNING这些)，就需要设定prompt encoder，同时调用`set_additional_trainable_modules`\n",
    "    - 只有PROMPT_TUNING/P_TUNING/PREFIX_TUNING才有prompt encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6e2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(pretrained_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846ce914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2359296 || all params: 1317935104 || trainable%: 0.17901458067543818\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()  # model == pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458bf91a",
   "metadata": {},
   "source": [
    "PeftModelForCausalLM\n",
    "- 继承自`PeftModel`\n",
    "- 初始化：\n",
    "    ```python\n",
    "    self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation\n",
    "    ```\n",
    "    从base model中可以看出来，这个函数返回的是\n",
    "    ```json\n",
    "    {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "     }\n",
    "\n",
    "    ```\n",
    "    这个函数好像只有在生成数据的时候使用，**EL**\n",
    "- 如果PeftConfig是Lora及其变体的话，base model是已经加入了初始化的low-rank decomposition weights（A/B矩阵）的LoraModel了，此时，预训练模型的部分参数已经被冻结；\n",
    "\n",
    "LoraModel\n",
    "- lora模型的forward和base model的forward是一样的，因为最终参与计算的这些权重是$$W = W_{0} + AB$$\n",
    "- Lora支持nn.Linear/nn.Embedding/nn.Conv1D\n",
    "- Lora的核心模块是`add_adapter`：\n",
    "    - self._prepare_lora_config(config, model_config)：\n",
    "        这个函数做了两个事情：\n",
    "        - 确定要做低秩分解的target_modules：配置参数指定，则根据指定来，如果没有指定，那么就按照内置的常用的模型来；\n",
    "        - 确定是否inference_mode，如果是，就合并权重；\n",
    "    - self._find_and_replace(adapter_name):\n",
    "        这个函数核心也就做了两个事情：\n",
    "        - find：找到需要加入lora的module，假设module为nn.Linear的话，生成与其对应的模块Linear，这个模块继承自nn.Linear和LoraLayer，nn.Linear与预训练中要被替换的线性层模块shape完全一致；LoraLayer生成对应配置的lora模块\n",
    "        - replace：将生成的模块Linear替换掉原来预训练模型中的nn.Linear，同时将原来预训练模型的nn.Linear的权重赋值给Linear的nn.Linear的模块；利用`setattr`将原来的模块替换成新模块。\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa5ec4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraModel(\n",
      "  (model)\n"
     ]
    }
   ],
   "source": [
    "print(str(model.base_model)[:20])  # AutoModelForCausalLM + Lora => LoraModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98dcfb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, base_model_name_or_path='EleutherAI/gpt-neo-1.3B', task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules=['q_proj', 'k_proj', 'v_proj'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True)}\n"
     ]
    }
   ],
   "source": [
    "print(model.peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f561877c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoForCausalLM(\n",
       "      (transformer): GPTNeoModel(\n",
       "        (wte): Embedding(50257, 2048)\n",
       "        (wpe): Embedding(2048, 2048)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x GPTNeoBlock(\n",
       "            (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTNeoAttention(\n",
       "              (attention): GPTNeoSelfAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (k_proj): Linear(\n",
       "                  in_features=2048, out_features=2048, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (v_proj): Linear(\n",
       "                  in_features=2048, out_features=2048, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): Linear(\n",
       "                  in_features=2048, out_features=2048, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPTNeoMLP(\n",
       "              (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "              (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "916c042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff31b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ee6914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814f69a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f058438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346ac119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(21106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "806d9998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0dae61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
