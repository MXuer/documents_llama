{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd478585",
   "metadata": {},
   "source": [
    "# Chinese-Vicuna中的对话数据处理方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadbdc0c",
   "metadata": {},
   "source": [
    "我们看到的微调，比如说 [standard_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，或者[alpaca-lora](https://github.com/tloen/alpaca-lora)，这些都是单轮的对话，也就是说，我问一个问题，微调后的模型回答一个问题。就完事了。\n",
    "\n",
    "这与唠嗑相差甚大，唠嗑是有上下文的关系的，如果模型在训练的时候只是给单轮对话的信息的话，就不是很适合唠嗑的场景。那么对于多轮对话的模型，我们该怎么去组织数据呢？\n",
    "\n",
    "本文记录的是基于[Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna)来看如何来组织数据训练一个可以多轮唠嗑的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d0f96",
   "metadata": {},
   "source": [
    "## 原始数据\n",
    "\n",
    "本文以 [BelleGroup/train_3.5M_CN](https://huggingface.co/datasets/BelleGroup/train_3.5M_CN/viewer/BelleGroup--train_3.5M_CN/train?row=2) 中的数据来举例子，这个数据量级很大，也包含多轮对话的数据。\n",
    "\n",
    "首先我们加载数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8b9004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duhu/anaconda3/envs/llm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset json (/home/duhu/.cache/huggingface/datasets/BelleGroup___json/BelleGroup--train_3.5M_CN-d0ea45919c9eb506/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations', 'id'],\n",
      "        num_rows: 3606402\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "data_belle = load_dataset(\"BelleGroup/train_3.5M_CN\")\n",
    "print(data_belle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12771b5",
   "metadata": {},
   "source": [
    "这个数据集一共有3606402条数据，举个例子看一条（选这一条是因为刚好这个数据集的第一条，不是多轮对话的方式……所以就顺位选了第二条），这一条也将作为我们的demo数据，来演示多轮对话的时候，输入到模型里面长什么样子。这个数据一共有两轮对话，而且两轮对话之间关系十分密切。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343c9572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"给定一段文本和关键词列表，删除文本中包含所有给定关键词的子字符串。\\n文本：\\\"这是一个测试句子，目的是看看模型是否可以正确地从这个句子中删除关键词。\\\"\\\\n关键词列表：[‘测试’，‘模型’]\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"assistant\",\n",
      "      \"value\": \"删除包含所有给定关键词的子字符串后，文本变为：\\\"这是一个句子，目的是看看是否可以正确地从这个句子中删除关键词。\\\"\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"好的。现在请你将这个文本中的所有的逗号都替换成空格。\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"assistant\",\n",
      "      \"value\": \"好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\\\"这是一个句子 目的是看看是否可以正确地从这个句子中删除关键词。\\\"。处理结果如何？\"\n",
      "    }\n",
      "  ],\n",
      "  \"id\": \"16012449\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "demo_data = [data_belle['train'][1]]\n",
    "data = Dataset.from_list(demo_data)\n",
    "print(json.dumps(data[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db69db",
   "metadata": {},
   "source": [
    "## Chinese-Vicuna的处理方式\n",
    "\n",
    "我们分别来看下[Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna)是如何处理这一条数据的。\n",
    "\n",
    "本文主要关注于如何处理原始数据，生成可以用来训练多轮对话模型的模型输入，再加上不同的仓库的输入数据格式也不大一样，和我们举的例子也不大一样，所以和源码相比，可能会有一些变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfed62e",
   "metadata": {},
   "source": [
    "\n",
    "微调训练的代码是：[**`finetune_chat.py`**](https://github.com/Facico/Chinese-Vicuna/blob/master/finetune_chat.py)\n",
    "\n",
    "生成`prompt`的代码是[**`prompt.py`**](https://github.com/Facico/Chinese-Vicuna/blob/master/prompt.py)中的`chat_prompt`。\n",
    "\n",
    "在使用指令数据进行微调的时候，一般都会有一个模板，将对话的信息镶嵌到模板中去，之后将镶嵌后的整体信息送入模型进行训练。\n",
    "\n",
    "先来看下 Chinese-Vicuna 的对话模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62ce428",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_pre = (\n",
    "    \"The following is a conversation between an AI assistant called Assistant and a human user called User. \"\n",
    "    \"The assistant is intelligent, knowledgeable and polite to answer questions of user.\\n\\n\"\n",
    ")\n",
    "prompt_history = \"User:{input}\\n\\nAssistant:{output}\\n\\n\"\n",
    "prompt_post = \"User:{input}\\n\\nAssistant:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2339af",
   "metadata": {},
   "source": [
    "这里面有三个变量，第一个就是个固定模板，跟开场白一样的东西，第二个是用来表征历史对话信息的，第三个就是最后一轮对话的输入的问题的一个模板，因为涉及到训练和推理这两个阶段，训练的时候，会把最后一轮对话的回答补上去，但是推理的时候就不会。\n",
    "\n",
    "接下来就是具体的操作了，let's 开干。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8efa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point, stage='train'):\n",
    "    user_prompt = prompt_pre # 固定开场白\n",
    "    # 这里面的字段是conversions，而不是input，因为上面的例子的字段是conversations\n",
    "    conversations = data_point['conversations']\n",
    "    # 获取多轮对话的轮数\n",
    "    assert len(conversations) % 2 == 0, f\"{data_point} not compeleted finised the conversation\"\n",
    "    num_turns = len(conversations) // 2\n",
    "    for i in range(num_turns - 1): # 最后一轮对话单独处理，此处不处理\n",
    "        assert conversations[i]['from'] == \"human\"\n",
    "        assert conversations[i+1]['from'] == \"assistant\"\n",
    "        human = conversations[i]['value']\n",
    "        assistant = conversations[i+1]['value']\n",
    "        user_prompt += prompt_history.format_map({'input': human, 'output': assistant})\n",
    "    # 添加最后一轮对话的输入部分\n",
    "    user_prompt += prompt_post.format_map({'input': conversations[2*num_turns-2]['value']})\n",
    "    # 根据是训练还是推理，用不同的方式来处理最后一轮对话的回答部分\n",
    "    if stage == 'train':\n",
    "        user_prompt += conversations[2*num_turns-1]['value']\n",
    "    \n",
    "    return {\"prompt\": user_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97b6aa",
   "metadata": {},
   "source": [
    "**训练的时候镶嵌到模板中的输入文本是这样的：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e634524f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a conversation between an AI assistant called Assistant and a human user called User. The assistant is intelligent, knowledgeable and polite to answer questions of user.\n",
      "\n",
      "User:给定一段文本和关键词列表，删除文本中包含所有给定关键词的子字符串。\n",
      "文本：\"这是一个测试句子，目的是看看模型是否可以正确地从这个句子中删除关键词。\"\\n关键词列表：[‘测试’，‘模型’]\n",
      "\n",
      "Assistant:删除包含所有给定关键词的子字符串后，文本变为：\"这是一个句子，目的是看看是否可以正确地从这个句子中删除关键词。\"\n",
      "\n",
      "User:好的。现在请你将这个文本中的所有的逗号都替换成空格。\n",
      "\n",
      "Assistant:好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\"这是一个句子 目的是看看是否可以正确地从这个句子中删除关键词。\"。处理结果如何？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "fn_kwargs = {\"stage\": 'train'}\n",
    "data_train = data.map(generate_prompt, fn_kwargs=fn_kwargs)\n",
    "print(data_train[0]['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64679ba",
   "metadata": {},
   "source": [
    "**验证或者测试的时候，输入文本是这样的：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76ebdcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a conversation between an AI assistant called Assistant and a human user called User. The assistant is intelligent, knowledgeable and polite to answer questions of user.\n",
      "\n",
      "User:给定一段文本和关键词列表，删除文本中包含所有给定关键词的子字符串。\n",
      "文本：\"这是一个测试句子，目的是看看模型是否可以正确地从这个句子中删除关键词。\"\\n关键词列表：[‘测试’，‘模型’]\n",
      "\n",
      "Assistant:删除包含所有给定关键词的子字符串后，文本变为：\"这是一个句子，目的是看看是否可以正确地从这个句子中删除关键词。\"\n",
      "\n",
      "User:好的。现在请你将这个文本中的所有的逗号都替换成空格。\n",
      "\n",
      "Assistant:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "fn_kwargs = {\"stage\": 'val'}\n",
    "data_val = data.map(generate_prompt, fn_kwargs=fn_kwargs)\n",
    "print(data_val[0]['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a605570",
   "metadata": {},
   "source": [
    "\n",
    "所以我们看到，区别就在于最后一轮的Assistant的输出有或者没有，也就是说在模型验证/测试的时候，需要模型预测的就是最后一轮对话的回答。之前的历史对话，都将作为上下文信息，送入到模型中去，来更好的输出当前问题的回复。\n",
    "\n",
    "**推理的时候，怎么维护历史信息：**\n",
    "\n",
    "那么在推理的时候，历史信息是如何处理的呢？这部分的代码位于[chat.py](https://github.com/Facico/Chinese-Vicuna/blob/master/chat.py)，模型训练好了之后，用来进行对话推理的时候，历史信息的处理和训练的时候是类似的，维护一个列表叫做`history`，这也是个字典，操作起来的时候，就是将历史信息分别按照`User`和`Assistant`的角色拼起来，再镶嵌到模板里面，第一轮对话的时候，这个`history`是个空列表。还是用我们的样例数据来演示一下，那第一轮对话的时候，效果可能就是下面这个样子的：\n",
    "\n",
    "```shell\n",
    "User: 给定一段文本和关键词列表，删除文本中包含所有给定关键词的子字符串。\n",
    "文本：\"这是一个测试句子，目的是看看模型是否可以正确地从这个句子中删除关键词。\"\\n关键词列表：[‘测试’，‘模型’]\n",
    "Assistant: 删除包含所有给定关键词的子字符串后，文本变为：\"这是一个句子，目的是看看是否可以正确地从这个句子中删除关键词。\"\n",
    "```\n",
    "\n",
    "那么我们就可以将这一轮的输出填充到 `history` 里面去：\n",
    "\n",
    "```python\n",
    "history = [\n",
    "    {\n",
    "        \"user\": \"给定一段文本和关键词列表，删除文本中包含所有给定关键词的子字符串。\",\n",
    "        \"assistant\": \"删除包含所有给定关键词的子字符串后，文本变为：\\\"这是一个句子，目的是看看是否可以正确地从这个句子中删除关键词。\\\"\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "在获得下一轮对话的`user`的输入的时候，遍历这个`history`，把这些成对的问答挨个的镶嵌到上面我们提到的模板里面，最终呈现出来的效果和上面提到的验证/测试的时候的输入文本是一模一样的。这样也能保证训练和推理的时候，模型的输入保持一致。\n",
    "\n",
    "以上就是`Chinese-Vicuna`在处理训练一个多轮对话模型的时候，对数据的处理方式，以及在推理的时候，如何维护一个列表来记录历史信息，来当作下一轮对话的上下文信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dad710f",
   "metadata": {},
   "source": [
    "剩下的部分，就是如何利用`tokenizer`来获得模型需要的输入了。这块可以参考[Alpaca-Lora中的tokenizer：从原始输入到模型输入](https://zhuanlan.zhihu.com/p/630126960)最后的部分。\n",
    "\n",
    "不同的对话模型，对数据处理的方式虽然有些不同，但是基本的大框架大同小异，比如[`MOSS`](https://github.com/OpenLMLab/MOSS)或者[`Chinese-LLaMA-Alpaca`](https://github.com/ymcui/Chinese-LLaMA-Alpaca)。当然，具体到实际操作的时候，还是有很多的细节，比如说`eos`这个token的添加方式，开场白的模板和历史信息是否要参与训练等等，这些大家可能处理方式也不太一样。具体的效果，暂时也没有看到对比的信息或者资料。如果后续有时间，还是希望能做一些对比实验。或者已经有对比的，麻烦跟我说一说。\n",
    "\n",
    "数据的预处理，暂时就先到这里，后面涉及到数据的一些细节问题，如果在实验中碰到，到时候再写文记录了。\n",
    "\n",
    "再之后，想要写一下peft相关的内容，比如peft的整体的框架逻辑是怎么样的，不同的微调方式的理论和代码，以及他们的区别等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21659a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
